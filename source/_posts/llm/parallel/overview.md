为什么采用分布式并行？
1. 模型大，单卡的内存容量有限
2. 加速

## 并行方案
1. 数据并行 -- batch的切分
   model 每张卡拥有**完整**的模型参数
   input 每张卡计算**batch级的部分tensor级的全部**的输入
   > 仅训练，zero方案使每张卡拥有纵向的部分模型参数（这种方案有点类似流水线并行，前者参数通信，后者x通信，这两种方式可以同时使用）

2. 张量并行 -- 模型的横向切分
   model 每张卡拥有**部分**的模型参数
   input 每张卡计算**batch级的全部tensor级的部分或全部**的输入

3. 流水线并行 -- 模型的纵向切分
   model 每张卡拥有**部分**的模型参数
   input 每张卡计算**batch和tensor级全部**的输入

4. 序列并行
   主要是模型的attention部分，张量并行是在head的维度做切分，而序列并行是在seq的维度做切分