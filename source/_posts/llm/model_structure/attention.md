## attention算法结构层面
![alt text](../../../pic/kv_cache-2.png)
### MHA
multi head query attention
### MQA
multi query attention
### GQA
group query attention
### MLA
multi head latent attention
[这篇将mla很清晰](https://zhuanlan.zhihu.com/p/714686419)
[这篇是定量分析](https://zhuanlan.zhihu.com/p/714761319)